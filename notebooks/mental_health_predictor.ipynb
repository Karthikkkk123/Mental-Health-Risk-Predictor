{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50db6b2b",
   "metadata": {},
   "source": [
    "# Mental Health Risk Predictor\n",
    "\n",
    "This notebook implements a Mental Health Risk Predictor using Artificial Neural Networks (ANN). The model analyzes lifestyle data, digital behavior, sleep patterns, and survey responses to predict mental health risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5616e71",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "In this section, we'll load and explore the dataset from various sources including app usage, sleep patterns, and survey responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040259ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and process data\n",
    "def load_app_usage_data():\n",
    "    app_usage_df = pd.read_csv('E:/dataset/app_usage/app_usage_stats.csv')\n",
    "    return app_usage_df\n",
    "\n",
    "def load_sensing_data():\n",
    "    sensing_df = pd.read_csv('E:/dataset/sensing/sensing_data.csv')\n",
    "    return sensing_df\n",
    "\n",
    "def load_survey_data():\n",
    "    survey_df = pd.read_csv('E:/dataset/survey/survey_responses.csv')\n",
    "    return survey_df\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    app_usage_df = load_app_usage_data()\n",
    "    sensing_df = load_sensing_data()\n",
    "    survey_df = load_survey_data()\n",
    "    \n",
    "    print(\"Data loaded successfully!\")\n",
    "    print(f\"App usage data shape: {app_usage_df.shape}\")\n",
    "    print(f\"Sensing data shape: {sensing_df.shape}\")\n",
    "    print(f\"Survey data shape: {survey_df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436b2fa",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Preprocess the data by handling missing values, encoding categorical variables, and scaling numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(app_usage_df, sensing_df, survey_df):\n",
    "    # Merge datasets on user_id and timestamp\n",
    "    merged_df = pd.merge(app_usage_df, sensing_df, on=['user_id', 'timestamp'], how='outer')\n",
    "    merged_df = pd.merge(merged_df, survey_df, on=['user_id', 'timestamp'], how='outer')\n",
    "    \n",
    "    # Handle missing values\n",
    "    merged_df = merged_df.fillna(merged_df.mean(numeric_only=True))\n",
    "    \n",
    "    # Extract features\n",
    "    features = [\n",
    "        'screen_time',\n",
    "        'app_switches',\n",
    "        'social_media_time',\n",
    "        'sleep_duration',\n",
    "        'physical_activity',\n",
    "        'stress_level',\n",
    "        'anxiety_score',\n",
    "        'depression_score'\n",
    "    ]\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = merged_df[features]\n",
    "    y = merged_df['mental_health_risk']  # Assuming this is our target variable\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# Preprocess the data\n",
    "try:\n",
    "    X_scaled, y, scaler = preprocess_data(app_usage_df, sensing_df, survey_df)\n",
    "    print(\"Data preprocessing completed successfully!\")\n",
    "    print(f\"Feature matrix shape: {X_scaled.shape}\")\n",
    "    print(f\"Target vector shape: {y.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in preprocessing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d098cc",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "Build and train the ANN model for mental health risk prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f3603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_model(X_train.shape[1])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Add early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=10,\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07d209e",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate the model's performance using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Make predictions\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7239df3b",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Save the trained model and preprocessing objects for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and scaler\n",
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "model.save('E:/Deep learning/ANN/Mental_Health_Predictor/models/mental_health_predictor.h5')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'E:/Deep learning/ANN/Mental_Health_Predictor/models/scaler.joblib')\n",
    "\n",
    "print(\"Model and scaler saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
